{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset parquet (/Users/andrewwright/.cache/huggingface/datasets/parquet/plain_text-57edf78d6033ac9a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d15232126a04559b775e40cf0e5b7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "\n",
    "# Define model name and path\n",
    "model_name = \"bert-base-uncased\"  # Adjust as needed\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Load SpokenSQUAD dataset\n",
    "squad_dataset = load_dataset(\"squad\", name=\"spoken_squad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squad_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data function (example)\n",
    "def preprocess_function(examples):\n",
    "  #ids = examples[\"id\"]\n",
    "  question = examples[\"question\"]\n",
    "  context = examples[\"context\"]\n",
    "  #print(examples[\"answers\"][0][\"answer_start\"])\n",
    "  answer_start = [] \n",
    "  answer_end =[]\n",
    "  for _, ans in enumerate(examples[\"answers\"]):\n",
    "    #print(ans[\"answer_start\"])\n",
    "    ans_start = ans[\"answer_start\"][0]\n",
    "    answer_start.append(ans_start)\n",
    "    ans_end = ans_start + len(ans[\"text\"][0].split())\n",
    "    answer_end.append(ans_end)\n",
    "  #print(answer_start)\n",
    "  #print(examples[\"answers\"][0][\"text\"])\n",
    "  #print(answer_start[0])\n",
    "  #print(int(answer_start[0]) + len(examples[\"answers\"][0][\"text\"][0].split()))\n",
    "  #answer_end =   answer_start + len(examples[\"answers\"][0][\"text\"][0].split())  # Assuming single answer\n",
    "  \n",
    "  # Tokenize and convert to tensors\n",
    "  encoding = tokenizer(question, context, padding=\"max_length\", truncation=True , return_tensors=\"pt\")\n",
    "  start_positions = torch.tensor([answer_start], dtype=torch.long)\n",
    "  #print(start_positions.shape)\n",
    "  #print(start_positions.squeeze(0).shape)\n",
    "  end_positions = torch.tensor([answer_end], dtype=torch.long)\n",
    "  \n",
    "  return {\n",
    "      \"input_ids\": encoding[\"input_ids\"],\n",
    "      \"attention_mask\": encoding[\"attention_mask\"],\n",
    "      \"start_positions\": start_positions.squeeze(0),\n",
    "      \"end_positions\": end_positions.squeeze(0),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cf719aeaa94d1687ebe4153ef7df3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5686011962043b6aa8d224367d30c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training and validation data\n",
    "train_dataset = squad_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "validation_dataset = squad_dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, **kwargs)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=8, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and learning rate scheduler (adjust parameters as needed)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "def train(epochs, device):\n",
    "  for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch in train_dataloader:\n",
    "      # Access data from batch\n",
    "      input_ids = batch[\"input_ids\"]\n",
    "      print(input_ids)\n",
    "      attention_mask = batch[\"attention_mask\"]\n",
    "      start_positions = batch[\"start_positions\"]\n",
    "      end_positions = batch[\"end_positions\"]\n",
    "      model_args = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"start_positions\": start_positions,\n",
    "        \"end_positions\": end_positions\n",
    "    }\n",
    "      # Forward pass\n",
    "      outputs = model(**model_args)\n",
    "      loss = outputs.loss  # Access loss from model outputs\n",
    "\n",
    "      # Backward pass and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "    \n",
    "    # Perform validation after each epoch (optional)\n",
    "    evaluate(validation_dataloader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(dataloader, device):\n",
    "  model.eval()  # Set model to evaluation mode\n",
    "  \n",
    "  # Initialize variables for keeping track of metrics\n",
    "  f1 = 0\n",
    "  exact_match = 0\n",
    "  total = 0\n",
    "  \n",
    "  for batch in dataloader:\n",
    "    # Access data from batch\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    start_positions = batch[\"start_positions\"]\n",
    "    end_positions = batch[\"end_positions\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "      outputs = model(**batch)\n",
    "    \n",
    "    # Extract predictions\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Implement your answer prediction logic here (e.g., beam search)\n",
    "    # This example uses greedy decoding (replace with your preferred method)\n",
    "    predicted_start_positions = torch.argmax(start_logits, dim=-1)\n",
    "    predicted_end_positions = torch.argmax(end_logits, dim=-1)\n",
    "\n",
    "    # Calculate metrics (replace with your preferred evaluation method)\n",
    "    for i in range(len(batch)):\n",
    "      # Assuming single answer per example\n",
    "      predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][i][predicted_start_positions[i]:predicted_end_positions[i]+1]))\n",
    "      reference_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(batch[\"context\"][i][start_positions[i]:end_positions[i]+1]))\n",
    "\n",
    "      # Update metrics based on your chosen evaluation method (e.g., SQuAD)\n",
    "      # Here, a simple string matching example\n",
    "      if predicted_answer == reference_answer:\n",
    "        exact_match += 1\n",
    "      total += 1\n",
    "\n",
    "  # Calculate F1 score (modify based on your chosen evaluation method)\n",
    "  if total > 0:\n",
    "    f1 = exact_match / total\n",
    "\n",
    "  # Print evaluation metrics (modify to print desired metrics)\n",
    "  print(f\"F1 Score: {f1:.4f}, Exact Match: {exact_match}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "context\n",
      "question\n",
      "answers\n",
      "input_ids\n",
      "attention_mask\n",
      "start_positions\n",
      "end_positions\n",
      "[tensor([101, 101, 101, 101, 101, 101, 101, 101]), tensor([2000, 2054, 1996, 2054, 2054, 2043, 2129, 2054]), tensor([ 3183,  2003, 13546,  2003,  7719,  2106,  2411,  2003]), tensor([2106, 1999, 1997, 1996, 2006, 1996, 2003, 1996]), tensor([ 1996,  2392,  1996, 24665,  2327, 24105, 10289,  3679]), tensor([ 6261,  1997,  6730, 23052,  1997,  2932,  8214,  3076]), tensor([2984, 1996, 2540, 2012, 1996, 1997, 1005, 3259]), tensor([ 9382, 10289,  2012, 10289,  2364, 10289,  1055,  2012]), tensor([ 3711,  8214, 10289,  8214,  2311,  8214,  1996, 10289]), tensor([ 1999,  2364,  8214,  1029,  2012,  4088, 26536,  8214]), tensor([ 8517,  2311,  2003,   102, 10289,  4640, 17420,  2170]), tensor([1999, 1029, 3875, 6549, 8214, 1029, 2405, 1029]), tensor([10223,   102,  2000,  2135,  1029,   102,  1029,   102]), tensor([26371,  6549,  2029,  1010,   102,  2004,   102,  2004]), tensor([2605, 2135, 3252, 1996, 6549, 2012, 2004, 2012]), tensor([1029, 1010, 1029, 2082, 2135, 2087, 2012, 2087]), tensor([ 102, 1996,  102, 2038, 1010, 2060, 2087, 2060]), tensor([6549, 2082, 6549, 1037, 1996, 5534, 2060, 5534]), tensor([2135, 2038, 2135, 3234, 2082, 1010, 5534, 1010]), tensor([ 1010,  1037,  1010,  2839,  2038, 10289,  1010, 10289]), tensor([ 1996,  3234,  1996,  1012,  1037,  8214, 10289,  8214]), tensor([ 2082,  2839,  2082, 10234,  3234,  1005,  8214,  1005]), tensor([2038, 1012, 2038, 1996, 2839, 1055, 1005, 1055]), tensor([ 1037, 10234,  1037,  2364,  1012,  2493,  1055,  2493]), tensor([ 3234,  1996,  3234,  2311, 10234,  2448,  2493,  2448]), tensor([2839, 2364, 2839, 1005, 1996, 1037, 2448, 1037]), tensor([1012, 2311, 1012, 1055, 2364, 2193, 1037, 2193]), tensor([10234,  1005, 10234,  2751,  2311,  1997,  2193,  1997]), tensor([1996, 1055, 1996, 8514, 1005, 2739, 1997, 2739]), tensor([2364, 2751, 2364, 2003, 1055, 2865, 2739, 2865]), tensor([ 2311,  8514,  2311,  1037,  2751, 11730,  2865, 11730]), tensor([ 1005,  2003,  1005,  3585,  8514,  1012, 11730,  1012]), tensor([1055, 1037, 1055, 6231, 2003, 1996, 1012, 1996]), tensor([2751, 3585, 2751, 1997, 1037, 3157, 1996, 3157]), tensor([8514, 6231, 8514, 1996, 3585, 3076, 3157, 3076]), tensor([2003, 1997, 2003, 6261, 6231, 1011, 3076, 1011]), tensor([1037, 1996, 1037, 2984, 1997, 2448, 1011, 2448]), tensor([ 3585,  6261,  3585,  1012,  1996, 11730,  2448, 11730]), tensor([ 6231,  2984,  6231,  3202,  6261,  2421, 11730,  2421]), tensor([1997, 1012, 1997, 1999, 2984, 2093, 2421, 2093]), tensor([1996, 3202, 1996, 2392, 1012, 6399, 2093, 6399]), tensor([6261, 1999, 6261, 1997, 3202, 1010, 6399, 1010]), tensor([2984, 2392, 2984, 1996, 1999, 2119, 1010, 2119]), tensor([1012, 1997, 1012, 2364, 2392, 1037, 2119, 1037]), tensor([3202, 1996, 3202, 2311, 1997, 2557, 1037, 2557]), tensor([1999, 2364, 1999, 1998, 1996, 1998, 2557, 1998]), tensor([2392, 2311, 2392, 5307, 2364, 2547, 1998, 2547]), tensor([1997, 1998, 1997, 2009, 2311, 2276, 2547, 2276]), tensor([1996, 5307, 1996, 1010, 1998, 1010, 2276, 1010]), tensor([2364, 2009, 2364, 2003, 5307, 1998, 1010, 1998]), tensor([2311, 1010, 2311, 1037, 2009, 2195, 1998, 2195]), tensor([1998, 2003, 1998, 6967, 1010, 7298, 2195, 7298]), tensor([5307, 1037, 5307, 6231, 2003, 1998, 7298, 1998]), tensor([2009, 6967, 2009, 1997, 1037, 9263, 1998, 9263]), tensor([1010, 6231, 1010, 4828, 6967, 1012, 9263, 1012]), tensor([2003, 1997, 2003, 2007, 6231, 5625, 1012, 5625]), tensor([1037, 4828, 1037, 2608, 1997, 2004, 5625, 2004]), tensor([6967, 2007, 6967, 2039, 4828, 1037, 2004, 1037]), tensor([ 6231,  2608,  6231, 14995,  2007,  2028,  1037,  2028]), tensor([1997, 2039, 1997, 6924, 2608, 1011, 2028, 1011]), tensor([ 4828, 14995,  4828,  2007,  2039,  3931,  1011,  3931]), tensor([ 2007,  6924,  2007,  1996, 14995,  3485,  3931,  3485]), tensor([2608, 2007, 2608, 5722, 6924, 1999, 3485, 1999]), tensor([2039, 1996, 2039, 1000, 2007, 2244, 1999, 2244]), tensor([14995,  5722, 14995,  2310,  1996,  7326,  2244,  7326]), tensor([6924, 1000, 6924, 3490, 5722, 1010, 7326, 1010]), tensor([2007, 2310, 2007, 2618, 1000, 1996, 1010, 1996]), tensor([ 1996,  3490,  1996,  4748,  2310, 24105,  1996, 24105]), tensor([ 5722,  2618,  5722,  2033,  3490,  2932, 24105,  2932]), tensor([ 1000,  4748,  1000, 18168,  2618,  2003,  2932,  2003]), tensor([2310, 2033, 2310, 5267, 4748, 3843, 2003, 3843]), tensor([ 3490, 18168,  3490,  1000,  2033,  3807,  3843,  3807]), tensor([ 2618,  5267,  2618,  1012, 18168,  7058,  3807,  7058]), tensor([4748, 1000, 4748, 2279, 5267, 1998, 7058, 1998]), tensor([2033, 1012, 2033, 2000, 1000, 4447, 1998, 4447]), tensor([18168,  2279, 18168,  1996,  1012,  2000,  4447,  2000]), tensor([5267, 2000, 5267, 2364, 2279, 2022, 2000, 2022]), tensor([1000, 1996, 1000, 2311, 2000, 1996, 2022, 1996]), tensor([1012, 2364, 1012, 2003, 1996, 4587, 1996, 4587]), tensor([2279, 2311, 2279, 1996, 2364, 7142, 4587, 7142]), tensor([ 2000,  2003,  2000, 13546,  2311,  9234,  7142,  9234]), tensor([1996, 1996, 1996, 1997, 2003, 4772, 9234, 4772]), tensor([ 2364, 13546,  2364,  1996,  1996,  1999,  4772,  1999]), tensor([ 2311,  1997,  2311,  6730, 13546,  1996,  1999,  1996]), tensor([2003, 1996, 2003, 2540, 1997, 2142, 1996, 2142]), tensor([1996, 6730, 1996, 1012, 1996, 2163, 2142, 2163]), tensor([13546,  2540, 13546,  3202,  6730,  1012,  2163,  1012]), tensor([1997, 1012, 1997, 2369, 2540, 1996, 1012, 1996]), tensor([1996, 3202, 1996, 1996, 1012, 2060, 1996, 2060]), tensor([ 6730,  2369,  6730, 13546,  3202,  2932,  2060,  2932]), tensor([2540, 1996, 2540, 2003, 2369, 1010, 2932, 1010]), tensor([ 1012, 13546,  1012,  1996,  1996,  1996,  1010,  1996]), tensor([ 3202,  2003,  3202, 24665, 13546, 26536,  1996, 26536]), tensor([ 2369,  1996,  2369, 23052,  2003, 17420, 26536, 17420]), tensor([ 1996, 24665,  1996,  1010,  1996,  1010, 17420,  1010]), tensor([13546, 23052, 13546,  1037, 24665,  2003,  1010,  2003]), tensor([ 2003,  1010,  2003, 14042, 23052,  2207,  2003,  2207]), tensor([1996, 1037, 1996, 2173, 1010, 3807, 2207, 3807]), tensor([24665, 14042, 24665,  1997,  1037,  1037,  3807,  1037]), tensor([23052,  2173, 23052,  7083, 14042,  2095,  1037,  2095]), tensor([1010, 1997, 1010, 1998, 2173, 1998, 2095, 1998]), tensor([1037, 7083, 1037, 9185, 1997, 7679, 1998, 7679]), tensor([14042,  1998, 14042,  1012,  7083,  2006,  7679,  2006]), tensor([2173, 9185, 2173, 2009, 1998, 3076, 2006, 3076]), tensor([1997, 1012, 1997, 2003, 9185, 3906, 3076, 3906]), tensor([7083, 2009, 7083, 1037, 1012, 1998, 3906, 1998]), tensor([ 1998,  2003,  1998, 15059,  2009,  8266,  1998,  8266]), tensor([9185, 1037, 9185, 1997, 2003, 1012, 8266, 1012]), tensor([ 1012, 15059,  1012,  1996,  1037,  1996,  1012,  1996]), tensor([ 2009,  1997,  2009, 24665, 15059,  8514,  1996,  8514]), tensor([ 2003,  1996,  2003, 23052,  1997, 24803,  8514, 24803]), tensor([ 1037, 24665,  1037,  2012,  1996,  2003, 24803,  2003]), tensor([15059, 23052, 15059, 10223, 24665,  2405,  2003,  2405]), tensor([ 1997,  2012,  1997, 26371, 23052,  6604,  2405,  6604]), tensor([ 1996, 10223,  1996,  1010,  2012,  1012,  6604,  1012]), tensor([24665, 26371, 24665,  2605, 10223,  1996,  1012,  1996]), tensor([23052,  1010, 23052,  2073, 26371,  6399,  1996,  6399]), tensor([2012, 2605, 2012, 1996, 1010, 2031, 6399, 2031]), tensor([10223,  2073, 10223,  6261,  2605,  9671,  2031,  9671]), tensor([26371,  1996, 26371,  2984,  2073,  4772,  9671,  4772]), tensor([ 1010,  6261,  1010, 22353,  1996,  5426,  4772,  5426]), tensor([2605, 2984, 2605, 2135, 6261, 1010, 5426, 1010]), tensor([ 2073, 22353,  2073,  2596,  2984,  2007,  1010,  2007]), tensor([ 1996,  2135,  1996,  2000, 22353,  1996,  2007,  1996]), tensor([6261, 2596, 6261, 3002, 2135, 9718, 1996, 9718]), tensor([ 2984,  2000,  2984, 16595,  2596,  2405,  9718,  2405]), tensor([22353,  3002, 22353,  9648,  2000,  3679,  2405,  3679]), tensor([ 2135, 16595,  2135,  4674,  3002,  1998,  3679,  1998]), tensor([ 2596,  9648,  2596,  2061, 16595,  3701,  1998,  3701]), tensor([ 2000,  4674,  2000, 12083,  9648,  7316,  3701,  7316]), tensor([3002, 2061, 3002, 9711, 4674, 2118, 7316, 2118]), tensor([16595, 12083, 16595,  2271,  2061,  1998,  2118,  1998]), tensor([ 9648,  9711,  9648,  1999, 12083,  2060,  1998,  2060]), tensor([4674, 2271, 4674, 8517, 9711, 2739, 2060, 2739]), tensor([2061, 1999, 2061, 1012, 2271, 1010, 2739, 1010]), tensor([12083,  8517, 12083,  2012,  1999,  1998,  1010,  1998]), tensor([ 9711,  1012,  9711,  1996,  8517, 21121,  1998, 21121]), tensor([ 2271,  2012,  2271,  2203,  1012,  2011, 21121,  2011]), tensor([1999, 1996, 1999, 1997, 2012, 2493, 2011, 2493]), tensor([8517, 2203, 8517, 1996, 1996, 2013, 2493, 2013]), tensor([1012, 1997, 1012, 2364, 2203, 2119, 2013, 2119]), tensor([ 2012,  1996,  2012,  3298,  1997, 10289,  2119, 10289]), tensor([ 1996,  2364,  1996,  1006,  1996,  8214, 10289,  8214]), tensor([2203, 3298, 2203, 1998, 2364, 1998, 8214, 1998]), tensor([1997, 1006, 1997, 1999, 3298, 3002, 1998, 3002]), tensor([1996, 1998, 1996, 1037, 1006, 2984, 3002, 2984]), tensor([2364, 1999, 2364, 3622, 1998, 1005, 2984, 1005]), tensor([3298, 1037, 3298, 2240, 1999, 1055, 1005, 1055]), tensor([1006, 3622, 1006, 2008, 1037, 2267, 1055, 2267]), tensor([1998, 2240, 1998, 8539, 3622, 1012, 2267, 1012]), tensor([1999, 2008, 1999, 2083, 2240, 4406, 1012, 4406]), tensor([ 1037,  8539,  1037,  1017,  2008, 24105,  4406, 24105]), tensor([ 3622,  2083,  3622, 11342,  8539,  1998, 24105,  1998]), tensor([2240, 1017, 2240, 1998, 2083, 1996, 1998, 1996]), tensor([ 2008, 11342,  2008,  1996,  1017,  8514,  1996,  8514]), tensor([ 8539,  1998,  8539,  2751, 11342,  1010,  8514,  1010]), tensor([2083, 1996, 2083, 8514, 1998, 1996, 1010, 1996]), tensor([1017, 2751, 1017, 1007, 1996, 9718, 1996, 9718]), tensor([11342,  8514, 11342,  1010,  2751,  2003,  9718,  2003]), tensor([1998, 1007, 1998, 2003, 8514, 2019, 2003, 2019]), tensor([1996, 1010, 1996, 1037, 1007, 2981, 2019, 2981]), tensor([2751, 2003, 2751, 3722, 1010, 4772, 2981, 4772]), tensor([8514, 1037, 8514, 1010, 2003, 1998, 4772, 1998]), tensor([1007, 3722, 1007, 2715, 1037, 2515, 1998, 2515]), tensor([1010, 1010, 1010, 2962, 3722, 2025, 2515, 2025]), tensor([2003, 2715, 2003, 6231, 1010, 2031, 2025, 2031]), tensor([1037, 2962, 1037, 1997, 2715, 1037, 2031, 1037]), tensor([3722, 6231, 3722, 2984, 2962, 4513, 1037, 4513]), tensor([1010, 1997, 1010, 1012, 6231, 8619, 4513, 8619]), tensor([2715, 2984, 2715,  102, 1997, 2030, 8619, 2030]), tensor([2962, 1012, 2962,    0, 2984, 2151, 2030, 2151]), tensor([6231,  102, 6231,    0, 1012, 8368, 2151, 8368]), tensor([ 1997,     0,  1997,     0,   102, 15709,  8368, 15709]), tensor([ 2984,     0,  2984,     0,     0,  2013, 15709,  2013]), tensor([1012,    0, 1012,    0,    0, 1996, 2013, 1996]), tensor([ 102,    0,  102,    0,    0, 2118, 1996, 2118]), tensor([   0,    0,    0,    0,    0, 1012, 2118, 1012]), tensor([   0,    0,    0,    0,    0, 1999, 1012, 1999]), tensor([   0,    0,    0,    0,    0, 3055, 1999, 3055]), tensor([   0,    0,    0,    0,    0, 1010, 3055, 1010]), tensor([   0,    0,    0,    0,    0, 2043, 1010, 2043]), tensor([   0,    0,    0,    0,    0, 2070, 2043, 2070]), tensor([   0,    0,    0,    0,    0, 2493, 2070, 2493]), tensor([   0,    0,    0,    0,    0, 3373, 2493, 3373]), tensor([   0,    0,    0,    0,    0, 2008, 3373, 2008]), tensor([   0,    0,    0,    0,    0, 1996, 2008, 1996]), tensor([   0,    0,    0,    0,    0, 9718, 1996, 9718]), tensor([   0,    0,    0,    0,    0, 2211, 9718, 2211]), tensor([   0,    0,    0,    0,    0, 2000, 2211, 2000]), tensor([   0,    0,    0,    0,    0, 2265, 2000, 2265]), tensor([   0,    0,    0,    0,    0, 1037, 2265, 1037]), tensor([   0,    0,    0,    0,    0, 4603, 1037, 4603]), tensor([    0,     0,     0,     0,     0, 13827,  4603, 13827]), tensor([    0,     0,     0,     0,     0,  1010, 13827,  1010]), tensor([   0,    0,    0,    0,    0, 1037, 1010, 1037]), tensor([   0,    0,    0,    0,    0, 4314, 1037, 4314]), tensor([   0,    0,    0,    0,    0, 3780, 4314, 3780]), tensor([   0,    0,    0,    0,    0, 1010, 3780, 1010]), tensor([   0,    0,    0,    0,    0, 2691, 1010, 2691]), tensor([   0,    0,    0,    0,    0, 3168, 2691, 3168]), tensor([   0,    0,    0,    0,    0, 2001, 3168, 2001]), tensor([   0,    0,    0,    0,    0, 2405, 2001, 2405]), tensor([   0,    0,    0,    0,    0, 1012, 2405, 1012]), tensor([    0,     0,     0,     0,     0, 10655,  1012, 10655]), tensor([    0,     0,     0,     0,     0,  1010, 10655,  1010]), tensor([   0,    0,    0,    0,    0, 1999, 1010, 1999]), tensor([   0,    0,    0,    0,    0, 2494, 1999, 2494]), tensor([   0,    0,    0,    0,    0, 1010, 2494, 1010]), tensor([   0,    0,    0,    0,    0, 2043, 1010, 2043]), tensor([   0,    0,    0,    0,    0, 2060, 2043, 2060]), tensor([   0,    0,    0,    0,    0, 2493, 2060, 2493]), tensor([   0,    0,    0,    0,    0, 3373, 2493, 3373]), tensor([   0,    0,    0,    0,    0, 2008, 3373, 2008]), tensor([   0,    0,    0,    0,    0, 1996, 2008, 1996]), tensor([   0,    0,    0,    0,    0, 3259, 1996, 3259]), tensor([   0,    0,    0,    0,    0, 3662, 3259, 3662]), tensor([   0,    0,    0,    0,    0, 1037, 3662, 1037]), tensor([   0,    0,    0,    0,    0, 4314, 1037, 4314]), tensor([    0,     0,     0,     0,     0, 13827,  4314, 13827]), tensor([    0,     0,     0,     0,     0,  1010, 13827,  1010]), tensor([   0,    0,    0,    0,    0, 1996, 1010, 1996]), tensor([   0,    0,    0,    0,    0, 4603, 1996, 4603]), tensor([   0,    0,    0,    0,    0, 3259, 4603, 3259]), tensor([   0,    0,    0,    0,    0, 3493, 3259, 3493]), tensor([    0,     0,     0,     0,     0, 13631,  3493, 13631]), tensor([    0,     0,     0,     0,     0,  2253, 13631,  2253]), tensor([   0,    0,    0,    0,    0, 2046, 2253, 2046]), tensor([   0,    0,    0,    0,    0, 2537, 2046, 2537]), tensor([   0,    0,    0,    0,    0, 1012, 2537, 1012]), tensor([   0,    0,    0,    0,    0, 4445, 1012, 4445]), tensor([   0,    0,    0,    0,    0, 3259, 4445, 3259]), tensor([   0,    0,    0,    0,    0, 2003, 3259, 2003]), tensor([   0,    0,    0,    0,    0, 2405, 2003, 2405]), tensor([   0,    0,    0,    0,    0, 2004, 2405, 2004]), tensor([   0,    0,    0,    0,    0, 2411, 2004, 2411]), tensor([   0,    0,    0,    0,    0, 2004, 2411, 2004]), tensor([   0,    0,    0,    0,    0, 1996, 2004, 1996]), tensor([   0,    0,    0,    0,    0, 9718, 1996, 9718]), tensor([   0,    0,    0,    0,    0, 1025, 9718, 1025]), tensor([   0,    0,    0,    0,    0, 2174, 1025, 2174]), tensor([   0,    0,    0,    0,    0, 1010, 2174, 1010]), tensor([   0,    0,    0,    0,    0, 2035, 1010, 2035]), tensor([   0,    0,    0,    0,    0, 2093, 2035, 2093]), tensor([   0,    0,    0,    0,    0, 2024, 2093, 2024]), tensor([   0,    0,    0,    0,    0, 5500, 2024, 5500]), tensor([   0,    0,    0,    0,    0, 2000, 5500, 2000]), tensor([   0,    0,    0,    0,    0, 2035, 2000, 2035]), tensor([   0,    0,    0,    0,    0, 2493, 2035, 2493]), tensor([   0,    0,    0,    0,    0, 1012, 2493, 1012]), tensor([   0,    0,    0,    0,    0, 2633, 1012, 2633]), tensor([   0,    0,    0,    0,    0, 1010, 2633, 1010]), tensor([   0,    0,    0,    0,    0, 1999, 1010, 1999]), tensor([   0,    0,    0,    0,    0, 3500, 1999, 3500]), tensor([   0,    0,    0,    0,    0, 2263, 3500, 2263]), tensor([   0,    0,    0,    0,    0, 2019, 2263, 2019]), tensor([   0,    0,    0,    0,    0, 8324, 2019, 8324]), tensor([   0,    0,    0,    0,    0, 3485, 8324, 3485]), tensor([   0,    0,    0,    0,    0, 2005, 3485, 2005]), tensor([   0,    0,    0,    0,    0, 2576, 2005, 2576]), tensor([   0,    0,    0,    0,    0, 2671, 2576, 2671]), tensor([   0,    0,    0,    0,    0, 2470, 2671, 2470]), tensor([   0,    0,    0,    0,    0, 1010, 2470, 1010]), tensor([   0,    0,    0,    0,    0, 3458, 1010, 3458]), tensor([   0,    0,    0,    0,    0, 4331, 3458, 4331]), tensor([   0,    0,    0,    0,    0, 1010, 4331, 1010]), tensor([   0,    0,    0,    0,    0, 2081, 1010, 2081]), tensor([   0,    0,    0,    0,    0, 2049, 2081, 2049]), tensor([   0,    0,    0,    0,    0, 2834, 2049, 2834]), tensor([   0,    0,    0,    0,    0, 1012, 2834, 1012]), tensor([   0,    0,    0,    0,    0,  102, 1012,  102]), tensor([  0,   0,   0,   0,   0,   0, 102,   0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move model to GPU if available\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device \u001b[38;5;241m=\u001b[39m device)  \u001b[38;5;66;03m# Adjust number of epochs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m evaluate(validation_dataloader, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[161], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m   model_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_positions\u001b[39m\u001b[38;5;124m\"\u001b[39m: start_positions,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_positions\u001b[39m\u001b[38;5;124m\"\u001b[39m: end_positions\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     26\u001b[0m   loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss  \u001b[38;5;66;03m# Access loss from model outputs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311env/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311env/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1844\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1842\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1844\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m   1845\u001b[0m     input_ids,\n\u001b[1;32m   1846\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1847\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1848\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1849\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1850\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1851\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1852\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1853\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1854\u001b[0m )\n\u001b[1;32m   1856\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1858\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311env/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311env/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:970\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[0;32m--> 970\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU if available\n",
    "train(epochs=1, device = device)  # Adjust number of epochs\n",
    "evaluate(validation_dataloader, device = device)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"my_squad_bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model._parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
